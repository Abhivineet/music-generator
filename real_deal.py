#import music21
from music21 import converter, instrument, note, chord, stream
import glob
import numpy
import scipy
import matplotlib
from keras.models import Sequential
from keras.layers import LSTM, Dense, Activation, Dropout
from keras.utils import np_utils

notes = []
i=0
for file in glob.glob("chopin/*.mid"):
    i = i + 1
    parsed_file = converter.parse(file)

    print("Parsing file: " + str(i))

    segment = parsed_file.flat.notes

    for stuff in segment:
        if stuff.isNote:
            notes.append(str(stuff.pitch))
        elif stuff.isChord:
            l = ""
            for c in stuff.normalOrder:
                L = str(c) + "."
                l = l + L
            notes.append(l)

pitchnames = sorted(set(notes))

note_dict = dict()
int_dict = dict()
for (number, note) in enumerate(pitchnames):
    note_dict[note] = number
    int_dict[number] = note

seq=20
input = []
output = []
input_2 = []

for i in range(len(notes)-seq):
    seq_in = notes[i:i+seq]
    seq_out = notes[i+seq]
    temp = []
    input_2.append([note_dict[char] for char in seq_in])
    for note in seq_in:
        temp.append(note_dict[note])
    input.append(temp)
    output.append(note_dict[seq_out])

input = numpy.reshape(input, (len(input),seq,1))
input = input/float(len(pitchnames))
oh_output = (np_utils.to_categorical(output))
#oh_output = numpy.reshape(oh_output, (len(output),1))

model = Sequential()

model.add(LSTM(512, input_shape=(input.shape[1], input.shape[2]),return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.52))
model.add(LSTM(512))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(n_vocab))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(input, oh_output, epochs=20, batch_size=64, verbose=1)

start = numpy.random.randint(0, len(input)-1)
p_start = input[start]
p_output = []
check_arr = []
for n in range(500):
    p_input = numpy.reshape(p_start, (1,len(p_start),1))
    prediction = model.predict(p_input, verbose=1)
    #ref = prediction.index(1)
    ref_check = numpy.argmax(prediction)
    #if(ref==ref_check):
     #   check_arr.append(1)
    p_output.append(int_dict[ref_check])
    #p_start[len(p_start)] = ref_check
    p_start = numpy.append(p_start, ref_check)
    p_start = p_start[1:len(p_start)]

offset = 0
output_notes = []
import music21
# create note and chord objects based on the values generated by the model
for pattern in p_output:
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes)
        new_chord.offset = offset
        output_notes.append(new_chord)
        # pattern is a note
    else:
        new_note = music21.note.Note(pattern)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
    offset += 0.5

midi_stream = stream.Stream(output_notes)

midi_stream.write('midi', fp='test_output.mid')